{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "In machine learning, especially in Support Vector Machines (SVMs), kernels are used to implicitly map data from a lower-dimensional space to a higher-dimensional one, where it becomes easier to separate the data linearly. A polynomial kernel is a specific type of kernel function that represents the polynomial transformation of the input features.\n",
    "\n",
    "The polynomial kernel function can be written as:\n",
    "\n",
    "K(x,x )=(x‚ãÖx +c) d\n",
    "\n",
    "are two input vectors (data points),\n",
    "\n",
    "c is a constant that shifts the decision boundary (usually set to 0),\n",
    "\n",
    "d is the degree of the polynomial.\n",
    "The relationship between polynomial functions and kernel functions is that the polynomial kernel is essentially a mathematical transformation that makes non-linear decision boundaries in the input space become linear in a higher-dimensional feature space. By using a polynomial kernel, we are able to model complex relationships between data points while using a simple linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and testing set (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1, coef0=1)  # You can change the degree and coef0 values\n",
    "\n",
    "# Train the model on the training set\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Polynomial SVM: {accuracy:.2f}')\n",
    "\n",
    "# Plot the decision boundary (only for two features)\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create the contour plot\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=80)\n",
    "plt.title('SVM Decision Boundary with Polynomial Kernel (Degree=3)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "n Support Vector Regression (SVR), the epsilon (\n",
    "ùúñ\n",
    "œµ) parameter controls the margin of tolerance where no penalty is given for errors. In simple terms, the epsilon defines a margin of error within which predictions are considered acceptable and no penalty is applied.\n",
    "\n",
    "Increasing epsilon: This allows for a larger margin of error, meaning fewer points will be penalized for errors. As a result, the model becomes more lenient, and fewer support vectors may be selected. This generally reduces the model's complexity.\n",
    "Decreasing epsilon: This makes the model stricter by reducing the margin of error, resulting in more points falling outside the margin and being penalized. As a result, more support vectors are likely to be selected, and the model becomes more sensitive to individual data points.\n",
    "Thus, as \n",
    "ùúñ\n",
    "œµ increases, the number of support vectors typically decreases because the model is less strict about errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?\n",
    "\n",
    "Here‚Äôs an explanation of how each parameter works and when to adjust them:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "Linear Kernel: If the data is linearly separable, the linear kernel is the simplest and fastest choice.\n",
    "Polynomial Kernel: Suitable for cases where the data has non-linear relationships that can be captured by polynomial equations. Increasing the degree can allow the model to fit more complex patterns.\n",
    "RBF Kernel: It is a popular kernel that maps data into a higher-dimensional space where a linear separation is possible. It is useful for non-linear data.\n",
    "C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between achieving a low error on the training data and keeping the model simple (avoiding overfitting).\n",
    "High C: More importance on fitting the training data well, leading to fewer misclassifications, but may result in overfitting.\n",
    "Low C: Allows more slack in fitting the training data, leading to a simpler model with potentially higher bias but reduced variance.\n",
    "Epsilon Parameter (\n",
    "ùúñ\n",
    "œµ):\n",
    "\n",
    "The epsilon parameter controls the margin of tolerance.\n",
    "High epsilon: More points will be within the margin and won't contribute to the error term, which can reduce the number of support vectors but may cause underfitting.\n",
    "Low epsilon: More points will be considered errors, leading to more support vectors and potentially overfitting.\n",
    "Gamma Parameter:\n",
    "\n",
    "Gamma defines how far the influence of a single training example reaches.\n",
    "High gamma: It leads to a more flexible decision boundary, but can cause overfitting, as the model becomes sensitive to individual data points.\n",
    "Low gamma: The model becomes smoother and may underfit the data as it will generalize over broader regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment Steps for SVM Classifier Implementation\n",
    "\n",
    "# 1. Import the necessary libraries and load the dataset:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load a dataset (e.g., Iris)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "# 2. Split the dataset into training and testing set:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Preprocess the data:\n",
    "# Scaling the data for better performance in SVM\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4. Create an instance of the SVC classifier and train it on the training data:\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 5. Use the trained classifier to predict the labels of the testing data:\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the performance of the classifier:\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Tune the hyperparameters using GridSearchCV:\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# 8. Train the tuned classifier on the entire dataset:\n",
    "best_svm = grid_search.best_estimator_\n",
    "best_svm.fit(X, y)\n",
    "\n",
    "# 9. Save the trained classifier to a file for future use:\n",
    "joblib.dump(best_svm, 'svm_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
